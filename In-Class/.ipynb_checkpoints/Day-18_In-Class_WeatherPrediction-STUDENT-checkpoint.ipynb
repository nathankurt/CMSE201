{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"text-align: right;\"> &#9989; Nathan Kurt</p>\n",
    "\n",
    "#### <p style=\"text-align: right;\"> &#9989; Put your group member names here.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 18 In-Class: Weather Prediction\n",
    "\n",
    "<img src=\"https://picturecorrect-wpengine.netdna-ssl.com/wp-content/uploads/2010/05/bad-weather2.jpg\" width=300px>\n",
    "\n",
    "## Goals for today's in-class assignment\n",
    "\n",
    "* Apply linear regression or autoregression model to Weather data to make a prediction.\n",
    "\n",
    "## Assignment instructions\n",
    "\n",
    "Work with your group to complete this assignment. Instructions for submitting this assignment are at the end of the notebook. The assignment is due at the end of class.\n",
    "\n",
    "**Important**: The culmination of this assignment is create the most accurate temperature prediction you can for the temperatures on Thanksgiving, November 22nd, along with an appealing visualization. You are welcome to work as a group to submit your best possible prediction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "# Forecasting Weather Trends in Lansing\n",
    "\n",
    "## Background\n",
    "\n",
    "In this activity you will extend your skills as a data scientist from understanding the past to predicting the future. Because of its impact on our daily lives and its potential [enormous consequences for the future of our civilization](https://www.nytimes.com/interactive/2016/08/20/sunday-review/climate-change-hot-future.html), we will use weather data.\n",
    "\n",
    "There is an abundance of data on this subject available freely on the Internet. We'll load the data into this notebook using Pandas. The data you're using has been scraped from [Weather Underground](https://www.wunderground.com) using their API for querying their database (which you can find [here](https://www.wunderground.com/weather/api/d/docs) if you're curious). Once you load up the data, we will model it. The models we will use are very simple: looking at trends by assuming the data have some functional form (here, we assume it follows a polynomial).\n",
    "\n",
    "You will also continue to build upon your experience using one of NumPy's built-in \"black boxes\" for finding trends in a set of numbers (temperature data in our case). That functionality is called __`polyfit`__ and, provided you send it the information it wants in the form it wants, it will return to you the \"best\" polynomial for the data. Roughly speaking, \"best\" means that none of the data are too far away from the fit line. As before, you will need to understand exactly what it returns. As always, use the Internet to find any additional information you might need. Try searching using terms like \"polyfit python use\". You will find a lot of [other people](http://stackoverflow.com/questions/18767523/fitting-data-with-numpy) doing similar things. \n",
    "\n",
    "Our goal for today is to predict the weather in Lansing for the next month or so using past data. Specifically, your goal is to predict the temperature on Thanksgiving (November 22nd). We will use the last ~4+ years of data to do this!\n",
    "\n",
    "Let's get to it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1 Loading and inspecting the data\n",
    "\n",
    "You'll find a number of datasets on D2L for this assignment. The files that are called things like \"2014_temperature_data.csv\", which contain the temperature data for every day in that year (with the exception of 2018 for obvious reasons). When you load up the data, you should see values for the date and the following temperature values:\n",
    "\n",
    "* meantempi -- the mean temperature recorded for that date\n",
    "* mintempi -- the minimum temperature recorded for that date\n",
    "* maxtempi -- the maximum temperature recorded for that date\n",
    "\n",
    "Try loading up a dataset and make sure you understand the values.\n",
    "\n",
    "In the event that you're itching for even more weather data, we've also provide the _all_ of the information provided by Weather Underground in the \"20XX_weather_data.csv\" files. We encourage you to load up at least one of those datasets and take a look at all of the information you _could_ model.\n",
    "\n",
    "If you're interested in knowing how we scraped the data off of the web, talk to us after class!\n",
    "\n",
    "**Also**, make some plots of the data so that you understand what you're looking at and what the general trends are from year to year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll load up some Python modules and set some parameters for making higher quality plots.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "# uncomment the following line if you want to make higher quality plots than the default (PNG)\n",
    "set_matplotlib_formats('pdf', 'svg') \n",
    "from pandas.plotting import lag_plot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "# You'll probably need to load some other modules as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'2017_temperature_data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ab663d42b7a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load up the data and make some plots of it to understand what some of the visual trends are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2017_temperature_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'2017_temperature_data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Load up the data and make some plots of it to understand what some of the visual trends are\n",
    "vals = pd.read_csv(\"2017_temperature_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "## Part 2: Modeling weather data with linear regression\n",
    "\n",
    "Let's think about how to model the weather. As you certainly have realized, this is an incredibly difficult thing to do. But, what we will do here is to examine past data and use it to predict future trends. If we were looking at data over a fairly short time period (say January and February) and predicting March, we may only need to ask: how much is the temperature increasing or decreasing as we head into the early parts of spring? To do this, we could assume that there is a linear relationship with time, as in\n",
    "$$ T(t) = \\beta t + \\gamma,$$\n",
    "\n",
    "where $\\beta$ and $\\gamma$ can be obtained by looking at past data. But, you could argue: this model for the data is too simple, and we might instead try something like\n",
    "$$ T(t) = \\alpha t^2 + \\beta t + \\gamma,$$\n",
    "\n",
    "which has some curvature to it - it is a \"quadratic\" function (or, a \"parabola\" if you prefer to think in terms of its graph). The quadratic is nice because the function has more structure in it and one more parameter that can help us capture details in the data better. In general, functions like this are called \"polynomials\", and we could also consider a cubic (by adding yet another term with $t^3$). Lucky for us, Python, and its wonderful library NumPy, knows a lot about how to match polynomials to data!\n",
    "\n",
    "To do this, we use the library function \"__polyfit__\" from NumPy. All the gory details are [here](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html). This is a function someone else wrote for you - aren't you lucky! It is very common to grab library functions like this from the large Python ecosystem rather than doing everything yourself.\n",
    "\n",
    "**Task:** Use `polyfit` to fit a quadratic function to one of the datasets that contains a full year of information (2014, 2015, 2016, or 2017) and plot the best fit line along with the data. You're free to try fitting whichever temperature value you want (i.e. min, max, or mean). Then, try fitting a cubic function as well (i.e. one that uses $t^3$!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Now try fitting the 2018 data and then overplot the best fit line. Also extend your the best fit line to predict 10 days worth of \"future\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part 3 Looking for correlations in weather data\n",
    "\n",
    "Next, we are going to revisit a very powerful idea, __correlations__ in data. When you have made plots so far, you plot the quantity of interest (e.g. temperature, position) against an obvious parameter, such as space $x$ or time $t$. The question we wish to answer now is: if we have two variables $T_1$ and $T_2$ that are both collected at various times $t$, are there relations between the values of $T_1$ and $T_2$ that hold for all $t$? That is, if we knew something about $T_1$, would be able to predict something about $T_2$? Such relationships would be incredibly useful! This is a deep subject, and we will only scratch the surface here by making a plot.\n",
    "\n",
    "Notice that Matplotlib will accept two arrays of the same length and plot the second versus the first. So, suppose we plot $T_1$ versus $T_2$. What would that look like? If there are correlations, when $T_1$ goes up, $T_2$ is likely to go up; that is, they might appear to fall along a line. Let's try it!\n",
    "\n",
    "**Task** Make a plot for the most recent year of complete data: 2017. You should **plot the max temperatures versus the min temperatures**. What's the best matplotlib function to use for this type of plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python ecosystem has some amazing tools for performing computational modeling. We have already seen a few of these:\n",
    "* NumPy\n",
    "* Matplotlib\n",
    "* Pandas\n",
    "* Seaborn\n",
    "\n",
    "Today, we will explore more about __Seaborn__. As always, if you want to learn more, you can go to its [website](http://seaborn.pydata.org/). There is a huge amount there, and we will only use a little of it today. As you progress through the semester, you can explore more of the things that Seaborn can help you with. Seaborn has a an impressive collection of special types of plots that are a bit different from the ordinary types you have learned earlier in the course (e.g. pie, histogram, line), and Seaborn's plots are well suited to data-science problems. \n",
    "\n",
    "Here is an simple example that uses the [jointplot](http://seaborn.pydata.org/generated/seaborn.jointplot.html) function in Seaborn to plot a correlation (for more information with what you can do with `jointplot` scroll through the documentation page in the link). **Pay special attention to the `kind` parameter, what happens if you don't include this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import seaborn as sns # import Seaborn; everyone calls it \"sns\" so you might as well do that too\n",
    "x_fake, y_fake = np.random.randn(2, 80)\n",
    "sns.jointplot(x_fake, y_fake, kind = \"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine this plot carefully. **What types of information are immediately apparent to you? In what situation would you use plots of this type?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Put your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you will have many features in your data (highs, lows, averages, highest ever, lowest ever; lead, titanium, cromium, and so on.) [Click here](https://seaborn.pydata.org/generated/seaborn.pairplot.html) to see how Seaborn can be used to find correlations among several variables. These types of plots are [very much customizable](http://i.imgur.com/K3bUaas.png)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Remake your correlation plot for the 2017 data using Seaborn's `jointplot` function. Does it look like Seaborn was able to find a correlation? Then, **make a plot for one of the other complete years: 2014, 2015, or 2016.** Is the correlation similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking back at your plots\n",
    "\n",
    "**Question**: Do the values you plotted appear to be correlated? Are their any points that appear to be anomalous?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Put your answer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 Fitting weather data with autoregression\n",
    "\n",
    "Intuitively, when we predict the tomorrow's temperature, we will usually use the information about today's temperature. This leads to a so-called **autoregression** model:\n",
    "$$ T(t+1) = \\beta T(t) + \\gamma.$$\n",
    "_An autoregressive model is one where a value from a time series is regressed on previous values from that same time series._\n",
    "Now, let's make a plot visually checking the relationship between `maxtempi(t)` and `maxtempi(t + 1)`, which is the temperature of the day after the first one. Let's use the data from 2018. What type of plot should you choose? (Think back to the pre-class assignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it show strong a linear relationship? If so, how can we estimate it using `polyfit`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the autoregressive model to make a prediction for October 29th, using all of the data we have up to October 28th (which is the last date in the dataset). What's your prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we make a weather prediction 10 days beyond the end of the past data? (Hint: you may need to use loops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Think back to your linear regression model and compare it to your autoregression model. Which model do you prefer? And why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Put your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "<h2 align=center><font color='red'>Challenge!!! How well can you predict the weather?</font></h2>\n",
    "\n",
    "As a challenge, your group should try to produce an estimate for the **high** and **low** temperatures in Lansing on Thanksgiving (November 22nd). You can use any sort of model you prefer using what we've learned up to this point. The goal is to see how close your forecast end up being to the real values and produce a high quality plot that visualizes your prediction.\n",
    "\n",
    "Make sure your visualize really conveys your point! Use as many [features](http://matplotlib.org/1.3.1/users/annotations_guide.html) as you can to make your result informative, intuitive, and visually appealing. Save your plot as `yourname_forecast.pdf` and put it in the D2L dropbox for today's activity.\n",
    "\n",
    "If you run out of time to finalize your prediction in-class, **feel free to try finishing it off so that you can compare how close you end up getting to the real value!**\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.azquotes.com/picture-quotes/quote-prediction-is-very-difficult-especially-if-it-s-about-the-future-niels-bohr-3-7-0757.jpg\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Assignment wrap-up\n",
    "\n",
    "Fille out the survey and then submit your notebook and your prediction plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(\n",
    "\"\"\"\n",
    "<iframe \n",
    "\tsrc=\"https://goo.gl/forms/0i3Mc4WoeZEij6pJ3\" \n",
    "\twidth=\"800px\" \n",
    "\theight=\"600px\" \n",
    "\tframeborder=\"0\" \n",
    "\tmarginheight=\"0\" \n",
    "\tmarginwidth=\"0\">\n",
    "\tLoading...\n",
    "</iframe>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You're done! Submit the notebook to D2L along with your forecast plot!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
